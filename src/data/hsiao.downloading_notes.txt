# Downloading data

The data is on ENA: https://www.ebi.ac.uk/ena/data/view/PRJEB6358

Sample IDs are in the file names.

Metadata matching files to samples are in Table S3. 

ENA has 1658 samples. From Table S3, it looks like this is a mix of mouse and human
samples. Table S3 includes the following human 16S samples:

a. Human fecal cholera study V4-16S rRNA sequencing (364 samples, 7 patients). 
It looks like donors C and E have a lot of "expanded"/"extended" time points - 
not sure if these are technical/biological replicates, or if they are just more frequent samples.

f. Healthy adult controls V4-16S rRNA sequencing (49 samples, 12 participants)
* Subjects and associated samples have been previously described in birth cohort of twins, triplets and their parents reported in Subramanian et al., 2014.
I think these samples will be in the old paper...

g. Healthy children controls V4-16S rRNA sequencing (1291 samples, 996 are from Subramanian et al, 50 participants)
* Sample IDs indicated in bold have been previously described in Subramanian et al., 2014, while remaining samples are from a smaller subset of children that were sampled into their third year of postnatal life

I'll manually copy these tables into an Excel sheet with the "raw" metadata. These are in my donor-selection repo.

## Gut full list of ftp files

I need to see how many of the samples listed in these tables are in this ENA repo, and also prepare
a file with just those file names (so I don't bother with downloading all the unnecessary/non-human/non-16S
files).

First, let's get a list of all the  ftp files

```
cut -f 12 PRJEB6358.txt | tail -n +2| tr ';' '\n' > ftp_files.txt
```

Next, let's see how many of the samples from the metadata are in this list of ftp files.

### Full code to prepare ftp file list

```
import pandas as pd

with open('ftp_files.txt', 'r') as f:
	ftpfiles = f.readlines()

ftpfiles = [l.strip() for l in ftpfiles]

df_ftp = pd.DataFrame(columns=['file_name', 'sample', 'ftp_path'])
df_ftp['ftp_path'] = ftpfiles

# Get rid of ftp prefix
fnames = [l.split('/fastq/')[1] for l in ftpfiles]
df_ftp['file_name'] = fnames

# Remove shotgun files
df_ftp = df_ftp[~df_ftp['file_name'].str.contains('shotgun')]

samples = df_ftp['file_name'].str.rsplit('.', 2).str[0]
samples = [i.split('_run1_R1')[0] for i in samples]
samples = [i.split('_run1_R2')[0] for i in samples]
samples = [i.split('_run2_R1')[0] for i in samples]
samples = [i.split('_run2_R2')[0] for i in samples]
df_ftp['sample'] = samples

# Read in Table S3 (the samples we want)
tables3 = pd.read_excel('TableS3_human16S_tidy.xlsx')

# We want human 16S samples 
# Remember, some of these aren't in the data bc they're in other studies
wantsamples = tables3['Fecal SampleID*'].values

# We have more samples than we want, bc some are metagenomics and some are mouse
havesamples = df_ftp['sample']
havesamples = list(set(havesamples)) # some are duplicate bc of R1/R2

# Samples we want to get (i.e. that are in Table S3 and which we have data for)
tables3['have_data'] = [i in havesamples for i in tables3['Fecal SampleID*']]
getsamples = tables3.query('have_data == True')['Fecal SampleID*'].tolist()

# Add run1 or run2 column
def get_run(s):
     try:
         return s.split('_')[1].rsplit('_',1)[0]
     except:
         return np.nan  
  
df_ftp['run'] = df_ftp['file_name'].apply(get_run)

# Now we can grab the samples that we want to download
todownload = df_ftp.query('sample == @getsamples').query('run == "run1"')

# And write them to a file
with open('ftp_files.human_16S_run1.txt', 'w') as f:
     f.write('\n'.join(todownload['ftp_path'].values) + '\n')
```

And then we can download! From the raw_data directory: 

```
while read f
do
wget $f
done < ../ftp_files.human_16S_run1.txt
```

## Exploration of the data that we have

Look at what samples we have data for, compared to the samples included in Table S3:

```
In [77]: tables3.query('have_data == True').groupby('Group description').size()
Out[77]: 
Group description
a. Human fecal cholera study V4-16S rRNA sequencing    236
g. Healthy children controls V4-16S rRNA sequencing    239

In [82]: tables3.query('have_data == True')[['Group description', 'Subject ID']].drop_duplicates().groupby('Group description').size()
Out[82]: 
Group description
a. Human fecal cholera study V4-16S rRNA sequencing     7
g. Healthy children controls V4-16S rRNA sequencing    25
```

So I have data from 7 cholera (adult, I think) patients and 25 healthy child controls. But over 200 samples for each group total! That makes sense, some of the cholera patients had over 100 samples collected (I'm assuming they didn't sequence them all...)

That's 475 samples in total. Let's look at the data files now:

```
In [99]: df_ftp.query('sample == @getsamples').groupby('sample').size().reset_index().groupby(0).count()
Out[99]: 
   sample
0        
2       2
4     473
```

So it looks like I have 4 files for 473 of the samples and 2 files for 2 of the samples. This makes sense, there is one fwd and rev read per sample, and it looks like all but two of the samples were run twice (_run1 and _run2).

Okay, I don't really want to process ~2000 samples so let's just pick one run. Let's do run1 (wrt the two samples which aren't in both run - one is a healthy control in run2, the other is a C.diarrhea sample in run1.)

